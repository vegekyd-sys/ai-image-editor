import { streamText, generateText, tool, stepCountIs } from 'ai';
import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';
import { z } from 'zod';
import { generatePreviewImage, generateImageWithReferences } from './gemini';
import agentPrompt from './prompts/agent.md';
import enhancePrompt from './prompts/enhance.md';
import creativePrompt from './prompts/creative.md';
import wildPrompt from './prompts/wild.md';
import type { Tip } from '@/types';

// ---------------------------------------------------------------------------
// Model
// ---------------------------------------------------------------------------

const bedrock = createAmazonBedrock({
  region: process.env.AWS_REGION?.trim(),
  accessKeyId: process.env.AWS_ACCESS_KEY_ID?.trim(),
  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY?.trim(),
});
const MODEL = bedrock('us.anthropic.claude-sonnet-4-5-20250929-v1:0');

// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------

interface AgentContext {
  currentImage: string;    // base64 data URL â€“ updated after each generation
  originalImage?: string;  // base64 data URL â€“ the very first image, never changes
  projectId: string;
  /** Images generated during this run (base64). Streamed to frontend out-of-band. */
  generatedImages: string[];
}

export type AgentStreamEvent =
  | { type: 'status'; text: string }
  | { type: 'content'; text: string }
  | { type: 'new_turn' }  // signals start of a new assistant response (after tool result)
  | { type: 'image'; image: string }
  | { type: 'tool_call'; tool: string; input: Record<string, unknown> }
  | { type: 'done' }
  | { type: 'error'; message: string };

// ---------------------------------------------------------------------------
// System prompt (bundled via webpack asset/source)
// ---------------------------------------------------------------------------

function getAgentSystemPrompt(): string {
  return agentPrompt;
}

// ---------------------------------------------------------------------------
// Tools (Vercel AI SDK style, closure over AgentContext)
// ---------------------------------------------------------------------------

function createTools(ctx: AgentContext) {
  return {
    generate_image: tool({
      description: `Edit the current photo. Write a detailed English editPrompt and optionally set useOriginalAsBase.

--- DECIDING useOriginalAsBase ---
Before calling this tool, answer: does the user want to FIX the current version, or START FRESH from the original?
- Fix current (default, useOriginalAsBase=false): "å†è°ƒæ•´ä¸€ä¸‹" / "äººè„¸ä¸å¯¹" / "ä¿ç•™æ•ˆæœä½†..." / "å»æ‰æŸä¸ªå…ƒç´ "
- Start fresh (useOriginalAsBase=true): "Pçš„ä¸å¥½é‡æ–°åš" / "ä¸æ»¡æ„é‡æ¥" / "æ¢ä¸ªæ–¹å¼"

--- IMAGES SENT TO GEMINI ---
When useOriginalAsBase=false (default): Image 1 = current version (BASE), Image 2 = original (face reference only)
When useOriginalAsBase=true: only the original photo is sent (single image, start fresh)
When no originalImage exists: only current photo is sent (single image)

--- EDITPROMPT STRUCTURE ---
BASE: State which image is the foundation (omit if useOriginalAsBase=true â€” original is implicitly the base)
FACE (when people are present): Copy face from original exactly:
  - Large face (>10% of frame): "Restore/preserve each person's face to exactly match the original photo: copy the exact face shape, eye shape, nose, mouth, jaw line, skin tone and texture. Do NOT slim, beautify, enlarge eyes, or alter any facial feature."
  - Small face (<10% of frame): "CRITICAL: Faces are small. Leave ALL face areas completely untouched â€” do NOT sharpen, retouch, relight, or process any face region. Treat face areas as masked off."
EDIT: What to actually change, in detail.`,
      inputSchema: z.object({
        editPrompt: z.string().describe('Detailed English prompt following the BASE/REFERENCE/FACE/EDIT structure from agent.md.'),
        useOriginalAsBase: z.boolean().optional().describe('Set true when user wants to start fresh from the original photo (e.g. "Pçš„ä¸å¥½é‡æ–°åš"). Default false = use current version as base.'),
        aspectRatio: z.string().optional().describe('Target aspect ratio e.g. "4:5", "1:1", "16:9"'),
      }),
      execute: async ({ editPrompt, useOriginalAsBase, aspectRatio }) => {
        const hasOriginal = ctx.originalImage && ctx.originalImage !== ctx.currentImage;
        // Determine which image is the base
        const baseImage = (useOriginalAsBase && hasOriginal) ? ctx.originalImage! : ctx.currentImage;
        const refImage = hasOriginal ? (useOriginalAsBase ? ctx.currentImage : ctx.originalImage!) : null;

        console.log(`\nğŸ¨ [generate_image] base=${useOriginalAsBase ? 'ORIGINAL' : 'CURRENT'} hasRef=${!!refImage}\neditPrompt:\n${editPrompt}\n`);

        let result: string | null;
        if (useOriginalAsBase && hasOriginal) {
          // Start fresh from original â€” single image, no reference to current version
          result = await generatePreviewImage(ctx.originalImage!, editPrompt, aspectRatio);
        } else if (!useOriginalAsBase && hasOriginal) {
          // Edit current version, reference original for face/details
          result = await generateImageWithReferences(
            [
              { url: ctx.currentImage,   role: 'å½“å‰ç¼–è¾‘ç‰ˆæœ¬ã€ä¸»å›¾ã€‘â€” è¾“å‡ºå›¾ç‰‡å¿…é¡»ä»¥è¿™å¼ å›¾ä¸ºåŸºç¡€è¿›è¡Œä¿®æ”¹' },
              { url: ctx.originalImage!, role: 'åŸå›¾ã€äººè„¸å‚è€ƒã€‘â€” ä»…ç”¨äºè¿˜åŸäººè„¸ç»†èŠ‚ï¼Œä¿æŒä¸åŸå›¾äººè„¸å®Œå…¨ä¸€è‡´' },
            ],
            editPrompt,
            aspectRatio,
          );
        } else {
          result = await generatePreviewImage(ctx.currentImage, editPrompt, aspectRatio);
        }

        if (!result) {
          return { success: false as const, message: 'Image generation failed. Try a different prompt.' };
        }
        ctx.currentImage = result;
        ctx.generatedImages.push(result);
        return { success: true as const, message: 'Image generated successfully and shown to the user.' };
      },
    }),

    analyze_image: tool({
      description: 'See and analyze the current photo. Returns the image so you can view it directly with your vision capabilities.',
      inputSchema: z.object({
        question: z.string().optional().describe('Optional focus area for the analysis'),
      }),
      execute: async ({ question }) => {
        const base64Data = ctx.currentImage.replace(/^data:image\/\w+;base64,/, '');
        const mimeType = ctx.currentImage.startsWith('data:image/png') ? 'image/png' : 'image/jpeg';
        return { base64Data, mimeType, question };
      },
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      toModelOutput({ output }: { output: any }) {
        return {
          type: 'content' as const,
          value: [
            { type: 'media' as const, data: output.base64Data, mediaType: output.mimeType },
            {
              type: 'text' as const,
              text: output.question
                ? `Analyze the image above, focusing on: ${output.question}`
                : 'Analyze this image in detail for photo editing purposes.',
            },
          ],
        };
      },
    }),

  };
}

// ---------------------------------------------------------------------------
// Agent runner â€“ async generator yielding SSE events
// ---------------------------------------------------------------------------

// Used for initial upload analysis
const ANALYSIS_PROMPT_INITIAL = `æè¿°è¿™å¼ ç…§ç‰‡é‡Œçš„å†…å®¹ï¼Œ1-2å¥ï¼Œè¯­æ°”åƒæœ‹å‹åˆ†äº«ã€‚ç›´æ¥ä»ä¸»ä½“å¼€å§‹è¯´ï¼ˆ"ä¸€ä¸ª..."/"ç”»é¢é‡Œ..."ï¼‰ã€‚ç¦æ­¢ç”¨"æˆ‘æ¥çœ‹çœ‹"/"è®©æˆ‘çœ‹ä¸€ä¸‹"ç­‰ä»»ä½•é“ºå«è¯­ã€‚`;

// Used for post-edit analysis â€” acknowledges the edit context
const ANALYSIS_PROMPT_POSTEDIT = `På®Œå›¾äº†ï¼Œçœ‹çœ‹æ•ˆæœã€‚ä»¥"På®Œä¹‹åï¼Œ"å¼€å¤´ï¼Œç”¨1å¥è¯æè¿°ä¸€ä¸‹ç°åœ¨è¿™å¼ å›¾çš„æ•´ä½“æ•ˆæœå’Œæ°›å›´ã€‚ç¦æ­¢ç”¨"æˆ‘æ¥çœ‹çœ‹"ç­‰é“ºå«è¯­ï¼Œç›´æ¥è¯´ç»“æœã€‚`;

export async function* runMakaronAgent(
  prompt: string,
  currentImage: string,
  projectId: string,
  options?: { analysisOnly?: boolean; analysisContext?: 'initial' | 'post-edit'; tipReactionOnly?: boolean; originalImage?: string },
): AsyncGenerator<AgentStreamEvent> {
  const ctx: AgentContext = {
    currentImage,
    originalImage: options?.originalImage,
    projectId,
    generatedImages: [],
  };

  const allTools = createTools(ctx);
  let imagesSent = 0;
  let stepCount = 0;

  const analysisOnly = options?.analysisOnly ?? false;
  const tipReactionOnly = options?.tipReactionOnly ?? false;
  const maxSteps = analysisOnly ? 2 : tipReactionOnly ? 1 : 10;
  const analysisPrompt = options?.analysisContext === 'post-edit'
    ? ANALYSIS_PROMPT_POSTEDIT
    : ANALYSIS_PROMPT_INITIAL;

  // Determine which tools to expose
  // tipReactionOnly: no tools (text-only response)
  // analysisOnly: only analyze_image (agent uses tool to see the photo)
  // normal chat: all tools (generate_image + analyze_image)
  const tools = tipReactionOnly ? undefined : analysisOnly
    ? { analyze_image: allTools.analyze_image }
    : allTools;

  try {
    const result = streamText({
      model: MODEL,
      system: getAgentSystemPrompt(),
      messages: [{ role: 'user', content: analysisOnly ? analysisPrompt : prompt }],
      ...(tools ? { tools } : {}),
      ...(analysisOnly && tools ? { activeTools: ['analyze_image' as const] } : {}),
      stopWhen: stepCountIs(maxSteps),
      onStepFinish: () => { stepCount++; },
    });

    for await (const event of result.fullStream) {
      // â”€â”€ Text delta â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (event.type === 'text-delta') {
        yield { type: 'content', text: event.text };
        continue;
      }

      // â”€â”€ Tool call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (event.type === 'tool-call') {
        if (event.toolName === 'analyze_image') {
          const q = (event.input as { question?: string }).question;
          yield { type: 'status', text: q ? `åˆ†æå›¾ç‰‡ï¼š${q.slice(0, 25)}` : 'åˆ†æå›¾ç‰‡' };
        } else if (event.toolName === 'generate_image') {
          yield { type: 'status', text: 'ç”Ÿæˆå›¾ç‰‡ä¸­...' };
        }
        yield { type: 'tool_call', tool: event.toolName, input: event.input as Record<string, unknown> };
        continue;
      }

      // â”€â”€ Tool result â€” flush generated images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (event.type === 'tool-result') {
        while (imagesSent < ctx.generatedImages.length) {
          yield { type: 'image', image: ctx.generatedImages[imagesSent] };
          imagesSent++;
        }
        continue;
      }

      // â”€â”€ Error from stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (event.type === 'error') {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const err = (event as any).error;
        const errMsg = err instanceof Error ? err.message : String(err);
        yield { type: 'error', message: errMsg };
        return;
      }

      // â”€â”€ New step start (after tool result, model begins next turn) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (event.type === 'start-step' && stepCount > 0) {
        yield { type: 'new_turn' };
      }
    }

    // Flush remaining images
    while (imagesSent < ctx.generatedImages.length) {
      yield { type: 'image', image: ctx.generatedImages[imagesSent] };
      imagesSent++;
    }

    yield { type: 'done' };
  } catch (err) {
    yield { type: 'error', message: err instanceof Error ? err.message : String(err) };
  }
}

// ---------------------------------------------------------------------------
// Tips Skill: generate tips text using Claude (fast, ~2-3s vs Gemini ~15s)
// ---------------------------------------------------------------------------

const TIPS_JSON_FORMAT = `\n\nè¯·ä¸¥æ ¼ä»¥JSONæ•°ç»„æ ¼å¼å›å¤ï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š
[{"emoji":"1ä¸ªemoji","label":"ä¸­æ–‡3-6å­—åŠ¨è¯å¼€å¤´","desc":"ä¸­æ–‡10-25å­—çŸ­æè¿°","editPrompt":"Detailed English editing prompt","category":"enhance|creative|wild"}, ...]`;

const TIPS_PROMPTS: Record<'enhance' | 'creative' | 'wild', string> = {
  enhance: enhancePrompt,
  creative: creativePrompt,
  wild: wildPrompt,
};

export async function* streamTipsWithClaude(
  imageBase64: string,
  category: 'enhance' | 'creative' | 'wild',
): AsyncGenerator<Tip> {
  const dataUrl = imageBase64.startsWith('data:') ? imageBase64 : `data:image/jpeg;base64,${imageBase64}`;
  const template = TIPS_PROMPTS[category];

  const systemPrompt = `ä½ æ˜¯å›¾ç‰‡ç¼–è¾‘å»ºè®®ä¸“å®¶ã€‚åˆ†æå›¾ç‰‡åç”Ÿæˆ2æ¡${category}ç¼–è¾‘å»ºè®®ã€‚labelå¿…é¡»ç”¨ä¸­æ–‡3-6å­—ï¼ŒåŠ¨è¯å¼€å¤´ã€‚editPromptç”¨è‹±æ–‡ï¼Œæå…¶å…·ä½“ã€‚`;

  const userPrompt = `åœ¨ç”Ÿæˆå»ºè®®ä¹‹å‰ï¼Œå…ˆåˆ†æè¿™å¼ å›¾ç‰‡ï¼šåˆ¤æ–­äººè„¸å¤§å°ï¼ˆå¤§è„¸>10% / å°è„¸<10%ï¼‰ï¼›è¯†åˆ«ç”»é¢ä¸­çš„å…·ä½“ç‰©å“/é£Ÿç‰©/é“å…·ï¼›åˆ¤æ–­ç…§ç‰‡æƒ…ç»ªåŸºè°ƒã€‚

åŸºäºåˆ†æï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹æ‰€æœ‰è§„åˆ™ï¼Œç»™å‡º2æ¡${category}ç¼–è¾‘å»ºè®®ï¼š

${template}${TIPS_JSON_FORMAT}`;

  const { textStream } = streamText({
    model: MODEL,
    system: systemPrompt,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', image: dataUrl },
          { type: 'text', text: userPrompt },
        ],
      },
    ],
  });

  // Collect full text then parse JSON
  let fullText = '';
  for await (const delta of textStream) {
    fullText += delta;
  }

  // Extract JSON array from response
  const jsonMatch = fullText.match(/\[[\s\S]*\]/);
  if (!jsonMatch) return;

  try {
    const tips = JSON.parse(jsonMatch[0]) as Tip[];
    for (const tip of tips) {
      if (tip.label && tip.editPrompt && tip.category) {
        yield tip;
      }
    }
  } catch { /* parse error, yield nothing */ }
}
